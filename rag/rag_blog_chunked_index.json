{
  "index_name": "blog_chunked_index",
  "documents": [
    {
      "text": "AKS provides multiple scaling dimensionsâhorizontal pod scaling, vertical resource adjustment, cluster autoscaling, event-driven scaling, and cross-cluster distributionâeach addressing different aspects of workload management. However, understanding when to apply each mechanism, how they interact, and their respective trade-offs remains challenging for platform teams navigating production deployment decisions.\nUnderstanding the capabilities, limitations, and optimal use cases for each AKS scaling mechanism enables informed architectural decisions aligned with specific workload characteristics and operational requirements. The solution requires evaluating six primary scaling approaches and their strategic combinations.\nQuick Decision Framework\nNeed application-level autoscaling\n(based on CPU, memory, or custom metrics)? Use\nHPA (Horizontal Pod Autoscaler)\nNeed event-driven, scale-to-zero behavior\n(with HPA integration).\nNeed cluster/VM-level autoscaling\nfor pod resource pressure? Use\nCluster AutoScaler (CAS)\nfor VMSS-based pools.\nNeed right-sized node provisioning\nwith less operational overhead? Use\nNAP (Node Auto Provisioning)\nâ AKS-managed Karpenter.\nNeed to tune pod resource requests automatically\nVPA (Vertical Pod Autoscaler)\nin recommendation mode.\nScaling across regions or many clusters\nAzure Kubernetes Fleet Manager\nAKS Scaling Repository\noffers comprehensive implementation guides, complete with validated examples and step-by-step instructions. It emphasizes the underlying mechanics and design rationale, enabling technical readers to understand how the solution worksânot just how to use it. Reviewing this repository will provide deep insights into the implementation details.\nHorizontal Pod Autoscaler (HPA)\nHPA automatically adjusts replica counts for Deployments or ReplicaSets based on observed metrics (CPU utilization, memory consumption, or custom application metrics). It operates at the application layer, scaling horizontally to distribute load across multiple pod instances. HPA requires Metrics Server for resource-based scaling or Prometheus adapter for custom metrics. On any brand new AKS cluster, metrics server is available/installed.\n: Cannot scale to zero (minimum replica typically â¥1), requires reliable metrics pipeline, and depends on sufficient cluster capacity for new pods.\nVertical Pod Autoscaler (VPA)\nVPA analyzes historical resource consumption patterns and recommends (or automatically applies) adjustments to pod resource requests and limits. Rather than scaling replica counts, VPA right-sizes individual pods to match actual resource needs, preventing over-allocation waste or under-allocation performance issues. VPA operates in three modes: recommendation-only (safe for production), auto-update (applies changes during pod restarts), or recreate (proactively restarts pods to apply changes). Best used in recommendation mode for production workloads, with changes applied during maintenance windows.\nCritical consideration\n: VPA and HPA should generally not actively control the same pods simultaneously. Use VPA to establish appropriate resource requests while HPA manages replica scaling.\nKEDA (Kubernetes Event-Driven Autoscaling)\nKEDA extends Kubernetes autoscaling capabilities to event-driven architectures by enabling pod scaling based on external metrics and event sourcesâmessage queue depth (Service Bus, RabbitMQ, Kafka), blob storage counts, HTTP request rates, database metrics, and numerous other triggers. KEDA can scale workloads to zero replicas during idle periods (unlike HPA) and activates pods when events arrive. It integrates with HPA, translating external metrics into HPA-compatible scaling signals. Best suited for queue consumers, stream processors, scheduled batch jobs, webhook handlers, and any workload where scaling decisions depend on external system state rather than internal pod metrics.\nCluster AutoScaler (CAS)\nCluster AutoScaler manages node counts for pre-defined VM Scale Set (VMSS) node pools by observing pending pods that cannot be scheduled due to insufficient cluster capacity. When resource pressure is detected, CAS requests additional nodes from Azure, typically provisioning new VMs within few minutes. CAS operates at the infrastructure layer, scaling node pools up when capacity is needed and down when nodes become underutilized. Best suited for existing AKS deployments with predictable VM type requirements and teams comfortable managing multiple specialized node pools for different workload categories.\n: Requires pre-creating node pools with specific VM sizes, leads to bin-packing inefficiencies\nNode Auto Provisioning (NAP) â Managed Karpenter\nNAP represents Microsoft's fully managed implementation of the open-source Karpenter project, providing node-level right-sizing where the AKS control plane provisions VMs dynamically based on actual pod requirements. Unlike CAS, NAP eliminates the need to define multiple node pools upfrontâinstead, platform teams define policies (NodePool and AKSNodeClass CRDs) specifying constraints like VM families, capacity types (spot/on-demand), and resource limits.\n: NAP reduces complexity but provides fewer node-level configuration options compared to self-hosted Karpenter.\nAzure Kubernetes Fleet Manager\nFleet Manager provides centralized management for multiple AKS clusters through a hub-and-spoke architecture, enabling workload distribution and scaling across clusters, regions, or availability zones. Platform teams define resources once on the Fleet hub cluster and use ClusterResourcePlacement policies to propagate workloads to member clusters. Scaling operations performed at the hub level automatically distribute across selected member clusters based on placement rules (PickAll, PickN, PickFixed). Best suited for multi-region resilience strategies, workloads exceeding single-cluster capacity limits, centralized governance requirements, or organizations managing numerous AKS clusters.\nAKS Scaling Repository â Comprehensive Guides and Examples\nAzure Kubernetes Service Documentation\nKubernetes Autoscaling Documentation\nKEDA â Kubernetes Event-Driven Autoscaling\nKarpenter Documentation\nAzure Karpenter Provider\nAzure Kubernetes Fleet Manager",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/aks-scaling/",
        "title": "AKS Scaling Deep Dive â Understanding Your Options",
        "section": "Part 1 of 1",
        "word_count": 798
      }
    },
    {
      "text": "Choosing between Azure Kubernetes Service (AKS) and Azure Container Apps (ACA) represents one of the most significant architectural decisions organizations face when adopting containerized applications on Azure. This decision fundamentally impacts development workflows, operational responsibilities, scalability approaches, and long-term platform capabilities.\nUnderstanding the specifics of each platform is crucial for making informed architectural decisions. Rather than providing superficial feature comparisons, the solution requires deep technical analysis of capabilities, limitations, and operational implications that directly impact your organization's success with containerized applications.\nAKS vs ACA Comparison Repository\nprovides structured, detailed analysis across critical decision dimensions including service deployment models, networking architectures, security implementations, and operational overhead comparisons. Rather than advocating for one platform over another, it presents factual capabilities and trade-offs to support informed decision-making.\nEach comparison topic includes practical examples, architectural implications, and specific considerations that teams should evaluate based on their unique context. The goal is empowering customers with the detailed technical insights needed to make confident platform choices aligned with their organizational realities.\nThe choice between AKS and Azure Container Apps reflects fundamental architectural decisions about operational responsibility, platform flexibility, and development approaches. Rather than viewing these as competing platforms, they represent different points on the spectrum between maximum control and managed simplicity.\nThe platform choice should align with organizational capabilities, operational preferences, and application requirements rather than technical features alone. Organizations with strong Kubernetes expertise and complex requirements benefit from AKS's flexibility. Teams prioritizing rapid development cycles and reduced operational overhead find value in ACA's managed approach.\nBoth platforms continue evolving with new capabilities and integration patterns. The decision should consider not just current requirements but anticipated growth, skill development, and long-term architectural strategies.\nAs container adoption matures, organizations may leverage both platforms strategically - using AKS for complex applications requiring extensive Kubernetes ecosystem integration and ACA for containerized applications where teams prefer managed infrastructure with optional serverless scaling capabilities.\nIf you are a technical reader who wants to understand technical specifics, don't forget to review the\nAKS vs ACA Comparison Repository\n, which will be updated periodically to reflect current trends and platform evolutions.\nNote on AKS Automatic:\nMicrosoft recently announced AKS Automatic, which has reached general availability. For the purposes of this comparison, consider AKS Automatic as an automated variant of traditional AKS that reduces operational overhead while preserving Kubernetes API access. This offering trades some granular control for simplified management, positioning it between standard AKS and Azure Container Apps on the control-to-convenience spectrum. I will add more specifics to the repository, but for now, I wanted to provide my initial assessment.\nAKS vs ACA Comprehensive Comparison\nAzure Kubernetes Service Documentation\nAzure Container Apps Documentation\nKubernetes API Reference",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/aks-aca/",
        "title": "AKS vs Azure Container Apps - Understanding the Architectural Trade-offs",
        "section": "Part 1 of 1",
        "word_count": 443
      }
    },
    {
      "text": "Certificate management in modern distributed systems presents significant challenges that impact security, availability, and operational efficiency. Without proper automation tools like Certbot, cert-manager, or service mesh, organizations face critical issues:\nManual Certificate Management Problems:\n: Expired certificates cause application downtime due to manual renewal processes\nSecurity Vulnerabilities\n: Weak key generation, insecure storage, delayed revocations, and inconsistent policies\n: Constant monitoring, coordinated renewals, complex deployments, and troubleshooting across infrastructure\n: Difficulty maintaining audit trails, security standards, and incident response\nScalability Limitations\n: Impossible to manage across microservices, bottlenecks in deployment, configuration drift\nThe evolution from manual certificate management to full automation represents a fundamental shift toward security infrastructure that scales with modern distributed systems.\nFoundation: Certificate Basics and ACME Protocol\nDigital certificates enable encryption, authentication, and integrity verification in TLS communications. Each certificate contains subject information, issuer details, public keys, validity periods, and digital signatures that establish trust relationships.\nThe ACME (Automatic Certificate Management Environment) protocol defined in RFC 8555 revolutionizes certificate management through automated domain validation, standardized protocol flows, and built-in security features. This eliminates manual verification processes and enables programmatic certificate lifecycle management.\nAutomated Solutions: Certbot and cert-manager\nimplements ACME protocol for traditional server environments, providing automated certificate issuance, renewal, and web server integration. It supports multiple validation methods and offers plugin architectures for various deployment scenarios.\nbrings ACME automation natively to Kubernetes environments through Custom Resource Definitions (CRDs). It provides declarative certificate management, supports multiple Certificate Authorities, handles automatic lifecycle management, and integrates seamlessly with ingress controllers.\nAdvanced Implementations\nProduction environments often require multi-tier certificate architectures spanning public internet-facing certificates, organizational internal certificates, and service-specific certificates with varying validity periods and trust requirements.\nKey considerations include comprehensive monitoring with proactive alerting, security best practices for key management and validation, and deployment strategies supporting zero-downtime updates through blue-green and canary approaches.\nFor detailed implementation guidance, laboratory exercises, and configuration examples, explore the\nwhich provides comprehensive hands-on experience from certificate fundamentals through advanced automation techniques.\nWhile Certbot and cert-manager solve many certificate management challenges,\nservice mesh technology\nrepresents the next evolution in secure communication infrastructure, addressing fundamental limitations that traditional approaches cannot overcome.\nService Mesh: Important consideration\nprovide comprehensive certificate automation that transcends traditional tools:\nAutomatic Certificate Lifecycle Management:\nWorkload identity-based certificate issuance without manual configuration\nUltra-short certificate lifespans (hours/minutes) for enhanced security\nZero-configuration mutual TLS (mTLS) for all service communications\nSimplified Trust and Operations:\nAutomatic trust distribution and centralized certificate authority management\nPolicy-driven security enforcement across all services\nBuilt-in observability, traffic management, and security policy integration\nBeyond Traditional Certificate Management:\nService meshes address challenges that Certbot and cert-manager cannot:\nAutomatic service-to-service authentication and authorization\nNetwork microsegmentation with cryptographic identity\nCompliance automation through built-in security controls\nTrue zero-trust networking with dynamic policy enforcement\nThe progression from manual certificate management through ACME automation to service mesh integration represents a fundamental shift toward\nsecurity as a platform capability\n. As organizations adopt cloud-native architectures with hundreds of services, service meshes provide the only scalable approach to maintain security without operational overhead while ensuring consistent policies across diverse environments.\nWhile tools like Certbot and cert-manager remain foundational for managing public-facing certificates and traditional workloads, service meshes represent the next evolutionary step in securing distributed applications. Certbot is more server-centric, while cert-manager is Kubernetes-native. By abstracting certificate complexity and automating identity-based communication, service meshes offer a more scalable and resilient approach to workload security. With the advent of eBPF-based datapaths for pod-to-pod communication, there are now multiple ways to implement service mesh-like capabilities. For example, ACNS on AKS leverages eBPF with either Cilium or Retina to enforce Layer 7 policies and FQDN filtering without relying on sidecar proxies. The\nimplements various Istio-like service mesh capabilities. This marks a subtle but important shift from traditional service meshes like Istio, which historically depend on Envoy sidecars for traffic interception and policy enforcement.\nIstio has recently introduced Ambient mode, a new architecture that uses zTunnel to encrypt inter-pod traffic. This approach offloads certain responsibilities from Envoy, allowing zTunnel to handle transport security while Envoy continues to manage advanced routing and observability features. The Azure AKS engineering team is actively\nto integrate Ambient mode into managed Istio offerings, signaling a broader shift toward more efficient and flexible service mesh implementations.\nJust as Kubernetes standardized compute, network, and storage through interfaces like CRI, CNI, and CSI, service meshes serve as the API layer for upper-tier concernsâsuch as traffic segmentation, rate limiting, and zero-trust security. These\ncapabilities complement platform services like Azure API Management (APIM), which continue to serve as centralized gateways for cross-product APIs, while service meshes handle intra-cluster communication and policy enforcement.\nin the GitHub repo to gain insight into key application challenges that arise when service mesh capabilities are not fully leveraged.\nReady to explore these concepts hands-on?\nprovides comprehensive laboratory exercises guiding you through each stage of this evolution, from certificate fundamentals to advanced automation techniques.\nI tested these examples locally and on AKS.\nI have been exploring GitHub Copilot in Agent mode recently and have been leveraging this capability to produce most of the code and content. However, I do vet the content, test, and publish it myself to ensure accuracy and quality.\nThe content and opinions expressed in this blog post are based on my personal experience and are intended for educational purposes only. This information is not meant to be implemented in production environments as-is without proper evaluation and testing. The goal is to help readers understand the challenges and nuances of different certificate management approaches to make informed decisions based on their specific requirements and constraints. This content is purely for educational purposes as it relates to Azure services and it's not a recommendation to implement as is. Always consult with your security team and follow your organization's policies and best practices when implementing security solutions in production environments.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/certmanager-tls-servicemesh/",
        "title": "Certificates, End-to-End TLS, Cert manager and Service mesh",
        "section": "Part 1 of 1",
        "word_count": 968
      }
    },
    {
      "text": "The Model Context Protocol (MCP) opens up new possibilities for creating AI-powered applications that can interact with various data sources and services. However, understanding how to deploy MCP servers in cloud environments and integrate them with AI services can be challenging for developers new to the protocol. There's a need for practical examples that demonstrate MCP's capabilities while showcasing real-world deployment patterns on cloud platforms like Azure.\nAdditionally, what if there's a need to provide an interface that uses natural language to store and retrieve data on cloud-backed persistent volumes, such as Azure Files? Traditional file storage interfaces require users to navigate complex directory structures, remember exact file names, and use specific query syntax. This creates friction when users want to quickly store information or find content using conversational, natural language interactions. The challenge becomes even more complex when considering multi-user scenarios, remote access patterns, and the need for scalable cloud-native deployments.\nThis blog post explores a simple yet practical MCP use case called \"Memento\" - a personal memory storage system that allows users to store and retrieve notes, meeting summaries, and other content using natural language. We'll walk through how this MCP server is deployed on Azure Kubernetes Service (AKS) with Azure Files for persistent storage.\n: This example is designed for educational and demonstration purposes. Security considerations are intentionally simplified and will be addressed in future blog posts. This implementation is NOT suitable for production use without significant security enhancements.\nMemento is an MCP server that provides users with a natural language interface to:\nStore personal memories, notes, and content\nRetrieve information using natural language queries\nOrganize content with tags and metadata\nAccess data remotely from local workstations\nThe system demonstrates several key concepts:\nRemote MCP server deployment\nCloud-native storage integration\nNatural language processing with Azure OpenAI\nArchitecture Overview\nThe current implementation represents the evolution from a simple local file-based system to a comprehensive Kubernetes deployment:\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\nâ Local Workstation â\nâ âââââââââââââââââââ â\nâ â MCP Client â â\nâ â Interactive â âââââ HTTPS/SSE âââââ â\nâ â memento_mcp_ â â â\nâ â client_ â â â\nâ â interactive.py â â â\nâ âââââââââââââââââââ â â\nââââââââââââââââââââââââââââââââââââââââââââââ¼âââââââââââââ\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\nâ Azure Cloud Services â\nâ âââââââââââââââââââââââââââââââââââââââââââââââââââââ â\nâ â Azure Kubernetes Service (AKS) â â\nâ â âââââââââââââââââââ âââââââââââââââââââ â â\nâ â â Load Balancer â â Azure Files â â â\nâ â â â â Persistent â â â\nâ â â Public IP â â Volume â â â\nâ â â IP Whitelist â â â â â\nâ â âââââââââââââââââââ â /memento_ â â â\nâ â â â storage/ â â â\nâ â â â âââ alice/ â â â\nâ â â¼ â âââ bob/ â â â\nâ â âââââââââââââââââââ â âââ charlie/ â â â\nâ â â MCP Server Pod ââââââ¤ â â â\nâ â â â âââââââââââââââââââ â â\nâ â â memento_mcp_ â â â\nâ â â server.py â â â\nâ â â â â â\nâ â â 0.0.0.0:8000 â â â\nâ â âââââââââââââââââââ â â\nâ âââââââââââââââââââââââââââââââââââââââââââââââââââââ â\nâ âââââââââââââââââââ â\nâ â Azure OpenAI â â\nâ â Service â â\nâ â - GPT-4o-mini â â\nâ â - Tool Calling â â\nâ â - Chat API â â\nâ âââââââââââââââââââ â\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n1. MCP Client (Local)\nRuns on local workstation\nIntegrates with Azure OpenAI for natural language processing\nUses Server-Sent Events (SSE) for real-time communication\nSupports multi-user scenarios with user isolation\nBuilt using FastMCP framework\nDeployed as Kubernetes pods on AKS\nProvides memory operations: store, retrieve, search, delete\nHandles user isolation and session management\n3. Storage Layer (Azure Files)\nPersistent storage mounted to AKS pods\nMulti-user directory structure\nSupports concurrent access from multiple pods\nEach user gets isolated storage space\n4. AI Integration (Azure OpenAI)\nNatural language processing for memory operations\nTool calling capability for MCP functions\nGPT-4o-mini model for efficient processing\n: \"Store this meeting summary about the Q4 planning session\"\n: MCP Client processes the request with Azure OpenAI\n: Azure OpenAI generates appropriate tool calls (store_memory)\n: Client sends tool calls to AKS-hosted MCP Server via SSE\n: MCP Server writes data to Azure Files with user isolation\n: Success confirmation flows back to user\nKey Technical Learnings\nNetwork Binding Challenge\nOne interesting technical challenge was that FastMCP hardcodes localhost binding, making it unreachable from Kubernetes services. This was solved using Python monkey patching:\n# Monkey patch uvicorn's Config to force host binding\n# Listen on all interfaces\nFor stable SSE connections, the Kubernetes service uses ClientIP session affinity to ensure requests from the same client always reach the same pod:\nsessionAffinityConfig\nThe system maintains user isolation through directory-based separation in Azure Files:\nâ âââ 20250118_memory.txt\nâ âââ 20250118_memory.txt.meta\nThe system uses environment-driven configuration for flexibility:\nAZURE_OPENAI_ENDPOINT\nhttps://your-resource.openai.azure.com\nSecurity Considerations\n: This demo uses minimal security for educational purposes only.\nFuture Enhancements Required for Production\nUser authentication and authorization\nTLS/SSL encryption for data in transit\nLoad balancer with Private IP vs public IP\nAzure Files encryption at rest\nRBAC and fine-grained permissions\nComprehensive audit logging\n: This deployment is for demonstration and development purposes only.\nBuild and Push Container\nmemento-mcp-server:v1.0.5\nfile with AKS Load Balancer IP\npython memento_mcp_client_interactive.py\nThis Memento example serves as a foundation for learning MCP by progressively adding features:\n(Next Blog): Authentication, encryption, and secure access patterns\n: Enhanced search capabilities, content versioning\nMonitoring & Observability\n: Logging, metrics, and health checks\n: High availability, backup strategies, and operational excellence\nThe Memento MCP demo demonstrates the evolution from simple local file systems to comprehensive cloud-native deployments. Key learning outcomes include:\n: Kubernetes deployment with multiple replicas\n: Azure Files for durable, shared storage\nRemote Access Patterns\n: Secure communication from local clients to cloud services\n: User isolation in shared storage environments\n: Natural language interfaces with Azure OpenAI\nNetwork Considerations\n: Session affinity for stable long-lived connections\nThis foundation showcases MCP's potential in cloud-native environments while highlighting real-world challenges and practical solutions. The next blog post will build upon this foundation to address production-ready security patterns and operational excellence.\nFor a detailed implementation reference, check out the\narchitecture document\nFor source code and other files, check out the\ncomplete source code and documentation\nFor a sample demo video\nsample video link here",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/mcpusecaseonazure/",
        "title": "MCP Use Case on Azure - Memento Memory Storage Demo",
        "section": "Part 1 of 1",
        "word_count": 1058
      }
    },
    {
      "text": "What is Model Context Protocol (MCP)?\nModel Context Protocol (MCP) is an open standard introduced by Anthropic in November 2024 that provides a standardized way for AI models to interact with external tools, data sources, and services in real-time. Instead of AI models being limited to their training data, MCP enables them to access live information, perform calculations, manipulate files, and execute complex workflows through well-defined tool interfaces.\nThis material provides a comprehensive learning journey for MCP, starting from basic concepts and progressing to a production-ready integration with Azure OpenAI. The goal is to demonstrate how MCP can transform AI applications by enabling them to interact with real-world systems and data. It starts with simple local communication and gradually introduces network-based architectures, culminating in a full integration with Azure OpenAI's function calling capabilities.\nWhy MCP Matters for Azure OpenAI\nAzure OpenAI's function calling capabilities become significantly more powerful when combined with MCP servers. This integration allows AI models to:\nAccess real-time data\n- Live system information, current weather, database queries\nPerform complex operations\n- File operations, calculations, API calls\n- Persistent storage and workflow management\n- Multiple tools and services can be added without model retraining\nSolution Architecture\nThe MCP integration with Azure OpenAI follows a three-tier architecture:\nâââââââââââââââââââ HTTPS/JSON âââââââââââââââââââ HTTP/SSE âââââââââââââââââââ\nâ Azure OpenAI â âââââââââââââââº â MCP Client â âââââââââââââââº â MCP Server â\nâ (Cloud API) â Function â (Python Bridge) â Tool Calls â (Local Tools) â\nâ â Calling â â â â\nâââââââââââââââââââ âââââââââââââââââââ âââââââââââââââââââ\n- Provides AI intelligence and function calling capabilities\n- Acts as a bridge, translating between OpenAI functions and MCP tools\n- Hosts the actual tools and capabilities (calculations, file operations, system access)\nLearning Path Overview\nThe complete learning journey consists of three progressive steps:\nStep 1: Basic MCP Concepts\n: Learn fundamental MCP concepts with local communication\n: stdio (subprocess communication)\n: Tools, Resources, Prompts, JSON-RPC messaging\n: Learn network-based MCP using web standards\n: SSE (Server-Sent Events) over HTTP\n: Network accessibility, persistent connections, multi-client support\nStep 3: Azure OpenAI Integration\n: Production-ready AI integration with real-time tools\n: HTTPS (Azure OpenAI) + SSE (MCP Server)\nazure_openai_mcp_client.py\n: Function calling, tool bridging, interactive AI, multi-step workflows\nKey Features Demonstrated\nMathematical Operations\n- Real-time calculations with error handling\n- Read, write, and manage files persistently\n- Access live system metrics and data\n- Integration with external APIs (mock demonstration)\n- User guidance and capability discovery\nProduction-Ready Patterns\n- Graceful failure modes and user feedback\n- Local MCP servers with encrypted Azure OpenAI API\n- Stateful interactions across multiple requests\nInteractive Experience\n- Menu-driven exploration and learning\nReal-World Example Workflow\nHere's how a typical interaction works:\n1. User: \"Calculate 15 * 8 and save the result to a file\"\n2. Azure OpenAI analyzes request and calls MCP tools:\n â calculate(operation=\"multiply\", a=15, b=8)\n â save_text_file(filename=\"result.txt\", content=\"120\")\n3. MCP Server executes tools and returns results:\n â \"Result: 15.0 multiply 8.0 = 120.0\"\n â \"Successfully saved content to result.txt\"\n4. Azure OpenAI provides final response:\n \"I've calculated 15 Ã 8 = 120 and saved the result to result.txt\"\n# Install dependencies\nopenai mcp psutil python-dotenv\nInteractive Experience (Recommended)\n# Terminal 1: Start MCP server with enhanced tools\npython azure_mcp_server.py\n# Terminal 2: Run interactive Azure OpenAI client\npython azure_openai_mcp_client_interactive.py\nThe interactive client provides:\nMenu-driven interface\nReal-time tool execution\nwith detailed feedback\nExample conversations\nto understand capabilities\nEven without Azure OpenAI credentials, the system runs in demo mode showing exactly how the integration would work, making it perfect for learning and experimentation.\nArchitecture Benefits\nSeparation of Concerns\n: Azure OpenAI handles natural language understanding\n: MCP servers handle specialized operations\n: Python clients manage the bridge between systems\nScalability & Flexibility\n: Easy to add new capabilities without retraining models\n: MCP servers can run anywhere accessible via HTTP\n: MCP servers can be written in any language\n: Sensitive operations remain on your infrastructure\nEncrypted Communication\n: Azure OpenAI API uses HTTPS\n: You control which tools are available to AI models\nThis implementation demonstrates the complete spectrum from basic MCP concepts to production-ready Azure OpenAI integration. The progressive learning approach ensures you understand:\nFundamental MCP concepts\nand communication patterns\nNetwork-based architectures\nwith real-time tool capabilities\nInteractive user experiences\nthat combine AI intelligence with live data\nFor detailed implementation guides, complete code examples, and step-by-step tutorials, refer to the comprehensive documentation and working code samples:\nð Complete Documentation & Code:\nhttps://github.com/srinman/mcpdemo/blob/main/mcp_tutoring_basic_to_advanced.md\nThe repository includes:\nWorking code examples\nfor all three learning steps\nDetailed explanations\nof each component and concept\nfor hands-on experimentation\nfor real-world deployment\nand validation scripts\nModel Context Protocol bridges the gap between AI intelligence and real-world capabilities. This integration with Azure OpenAI demonstrates how modern AI applications can access live data, perform complex operations, and maintain state - transforming AI from a text generator into a capable assistant that can interact with your entire technology stack.\nThe three-step learning journey provides both conceptual understanding and practical implementation experience, enabling you to build sophisticated AI applications that combine the power of large language models with the flexibility of real-time tools and services.\nIn the next blog post, we will explore how to secure the MCP server using EntraID, APIM.\nhttps://phase2online.com/2025/04/28/what-is-an-mcp-server-explained/\nhttps://github.com/Azure-Samples/remote-mcp-apim-functions-python?tab=readme-ov-file\nhttps://techcommunity.microsoft.com/blog/microsoft-security-blog/understanding-and-mitigating-security-risks-in-mcp-implementations/4404667\nhttps://www.redhat.com/en/blog/model-context-protocol-mcp-understanding-security-risks-and-controls\nhttps://blog.christianposta.com/understanding-mcp-authorization-step-by-step-part-two/\nhttps://modelcontextprotocol.io/specification/2025-06-18/basic/authorization",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/mcponazure/",
        "title": "Model Context Protocol (MCP) - From Hello World to Azure OpenAI Integration",
        "section": "Part 1 of 1",
        "word_count": 875
      }
    },
    {
      "text": "Istio PeerAuthentication CRD helps to authenticate the calls between services in the mesh. However, if you want to authenticate the calls from external services, Istio requires a different CRD which is called RequestAuthentication. How can you configure RequestAuthentication CRD to verify EntraID JWT and allow/deny calls?\nThis CRD helps to authenticate the calls from external services using JWT tokens. This guide will show you how to use RequestAuthentication CRD to authenticate the calls from external services using JWT tokens.\nRequestAuthentication resource configures the Istio proxy with data extracted from a JWT token. The JWT token is extracted from the request header and the data is extracted from the token and stored as filter metadata. The data can be used in the authorization policy to allow or deny the request.\nRequestAuthentication and AuthenticationPolicy CRDs:\nAuthenticationPolicy CRD is similar to what is used for PeerAuthentication CRD. Since PeerAuthentication CRD is used to authenticate the calls between services in the mesh, the correspoonding AuthenticationPolicy CRD uses source: principals to specify the source of the call. However, RequestAuthentication CRD is used to authenticate the calls from external services, the corresponding AuthenticationPolicy CRD uses source: requestPrincipals to specify the source of the call.\nSteps to implement RequestAuthentication CRD to verify EntraID JWT and allow/deny calls are as follows:\nDefine 'RequestAuthentication' CRD to specify the JWT token issuer, jwksUri, and the JWT token audience. Change 'x' to your tenant id.\napiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\n name: reqauth-product-jwt\n namespace: aks-istio-ingress\n - issuer: \"https://sts.windows.net/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/\"\n jwksUri: \"https://login.microsoftonline.com/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/discovery/v2.0/keys\"\nDefine 'AuthorizationPolicy' CRD to verify JWT claims and allow/deny calls. The 'requestPrincipals' field is used to specify the source of the call. The 'principals' field is used to specify the source of the call in PeerAuthentication CRD. Change 'mytenant' to your tenant name and also change the 'upn' claim to the claim that you want to verify.\nYou can use the following command to get the upn claim.\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\n namespace: aks-istio-ingress\n requestPrincipals: [\"*\"]\n - key: request.auth.claims[upn]\n values: [\"admin@mytenant.onmicrosoft.com\"]\nFor testing this, you can use the following command to get the access token and use it in the curl command to test the call.\naz account get-access-token\nhttp://ingressgateway-ip-address\n\"Authorization: Bearer\nReplace ingressgateway-ip-address with the IP address of the ingress gateway. You can get the IP address of the ingress gateway using kubectl get svc -n aks-istio-ingress\nIf you change access token, the call will be denied.\nYou can optionally enable access logging to see the logs in the ingress gateway pod. The following command will enable access logging.\napiVersion: telemetry.istio.io/v1\n name: ns-logging-aks-istio-ingress\n namespace: aks-istio-ingress\nCheck logs in the ingress gateway pod.\naks-istio-ingressgateway-external\nYou should be able to see the logs with failed messages if the access token is not valid.\n- rbac_access_denied_matched_policy\nFor successful calls, you should see the logs with success messages.\nor if the call is made to a valid endpoint, you should see the logs with success messages.\n\"GET /productpage HTTP/1.1\"\nFor detailed examples and steps, please refer to the following links:\nhttps://github.com/srinman/aksworkshop/blob/main/lab-istio/istio-security.md\nhttps://istio.io/latest/docs/tasks/security/authorization/authz-jwt/",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/aksistiojwtauth/",
        "title": "Using EntraID JWT to verify external calls with Istio add-on on AKS",
        "section": "Part 1 of 1",
        "word_count": 496
      }
    },
    {
      "text": "Istio supports authorization policies that can be used to control access to services. If Istio's built-in authorization policies are not sufficient, you can use external authorization services. This guide will show you how to use an external authorization service with Istio on AKS.\nWith Istio add-on for AKS, you can use an external authorization service to control access to services. You can use a sidecar or a service to implement the external authorization service. This guide will show you how to implement an external authorization service as a sidecar and as a service.\nHigh level steps to implement external authz service as a centralized service are as follows:\nDefine a configmap with a name istio-shared-configmap-asm-x-yy (where x-yy is the version of Istio) and add the extension providers for envoyExtAuthzHttp and envoyExtAuthzGrpc.\nCreate a new deployment for the external authorization service. The deployment should have a container that runs the external authorization service.\nCreate a new service for the external authorization service. The service should have a selector that matches the labels of the external authorization service deployment.\nCreate a new Istio authorization policy that uses the external authorization service. The policy action should be CUSTOM and the provider should be the name of the extension provider that you defined in the configmap.\nHigh level steps to implement external authz service as a sidecar are as follows:\nDefine a configmap with a name istio-shared-configmap-asm-x-yy (where x-yy is the version of Istio) and add the extension providers for envoyExtAuthzHttp and envoyExtAuthzGrpc.\nInject the external authorization service as a sidecar in the deployment of the service that you want to protect. The sidecar should have a container that runs the external authorization service.\nCreate a new Istio authorization policy that uses the external authorization service. The policy action should be CUSTOM and the provider should be the name of the extension provider that you defined in the configmap.\nCreate a ServiceEntry for the external authorization service. The service entry should have the host as 127.0.0.1 for the external authorization service and the ports that the external authorization service uses. This redirects the external authorization service traffic to the sidecar.\nRefer to the following link for detailed steps to implement external authz service as a sidecar or a centralized service on AKS.\nhttps://github.com/srinman/aksworkshop/blob/main/lab-istio/istio-mesh-extauthz.md",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/aksistioexternalauth/",
        "title": "Using External Auth with Istio add-on on AKS",
        "section": "Part 1 of 1",
        "word_count": 379
      }
    },
    {
      "text": "In order to prove application identity to Azure API Management (APIM) or any other service, the application needs to present a token. The token can be obtained from Azure Active Directory (AAD) using a client secret or a certificate. However, managing and securing these secrets can be challenging, especially in a cloud-native environment. Workload identity provides a more secure and manageable way to authenticate applications to Azure services. What can be done to leverage workload identity to obtain a token and call APIM or any other service without managing secrets?\nThe blog discusses a specific use case on how to use Azure AD workload identity to authenticate Kubernetes workloads to Azure AD and obtain tokens for calling APIM endpoint. This approach eliminates the need for secrets by using federated credentials registered in the managed identity. APIM can validate these tokens, verify claims, and allow or reject requests based on the identity.\nRegister a dummy application in Azure AD\nThis application registration is for resource server in the context of OAuth 2.0. We will not be using this app registration for any other purpose.\nAPIM Policy Explanation\nhttps://github.com/srinman/azureexamples/blob/main/aks/workloadidentity/policysample.xml\nThis policy ensures that only requests with valid tokens from approved managed identities are allowed, enhancing security by verifying the identity of the caller.\nPython Program Explanation\nhttps://github.com/srinman/azureexamples/blob/main/aks/workloadidentity/pysdktokapim.py\nThe Python program demonstrates how to obtain an access token from Azure AD using the workload identity. This token can then be used to authenticate API calls to APIM.\nfor deploying this in AKS, follow steps as documented here.\nhttps://github.com/srinman/azureexamples/blob/main/aks/workloadidentity/README.md\nSummary of the Approach\nThis approach leverages Azure AD workload identity to securely call Azure API Management (APIM) endpoints from an Azure Kubernetes Service (AKS) cluster. Hereâs a detailed summary of the benefits:\nSecretless Authentication\nBy using Azure AD workload identity, the need for storing and managing secrets is eliminated. This enhances security by reducing the risk of secret leakage.\nFederated Credentials\nFederated credentials allow the AKS workloads to authenticate to Azure AD without needing to manage service principal credentials. This is achieved by registering a federated credential in the managed identity.\nThe Python program demonstrates how to obtain an access token from Azure AD using the workload identity. This token can then be used to authenticate API calls to APIM.\nAPIM Policy for Identity Verification\nThe APIM policy validates the JWT token included in the API request. It checks the tokenâs issuer, audience, and required claims to ensure that the request is coming from an approved managed identity.\nThis policy ensures that only requests with valid tokens from specific managed identities are allowed, enhancing security by verifying the identity of the caller.\nGranular Access Control\nBy specifying the audience, issuer, and required claims in the APIM policy, granular access control can be enforced. This ensures that only specific AKS clusters, namespaces, and service accounts can access the APIM endpoint.\nThis same container can be deployed in Azure Container Apps. You can associate the same user-assigned managed identity to the container app and use the same Python program to obtain the token and call APIM. You need to ensure that you set AZURE_CLIENT_ID and AZURE_TENANT_ID as environment variables in the container app. In AKS, these are automatically set by the mutating webhook.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/workloadidentity2apim/",
        "title": "Trusted identity but secretless call to APIM",
        "section": "Part 1 of 1",
        "word_count": 537
      }
    },
    {
      "text": "You have a web application running in Azure Container Apps, and you want to deploy a new version of the application. However, you want to ensure that the new version is stable and does not introduce any issues before fully rolling it out to all users. To achieve this, you want to perform a canary deployment, where a small percentage of traffic is routed to the new version while the majority of traffic continues to go to the existing version.\nTraffic Splitting in AKS\nIn Azure Kubernetes Service (AKS), traffic splitting is typically achieved using service meshes like Istio or Linkerd, or by leveraging Kubernetes-native tools like Ingress controllers. These tools allow you to define routing rules that split traffic between different versions of your application. While powerful, setting up and managing these tools can be complex and requires a deep understanding of Kubernetes networking and service mesh concepts.\nTraffic Splitting in Azure Container Apps\nAzure Container Apps simplifies traffic splitting by providing built-in support for this feature. With Azure Container Apps, you can easily define traffic splitting rules directly in your application configuration, without the need for additional infrastructure or complex setups.\nUse Case: Canary Deployment for a Web Application\nSolution: Leveraging Labels and Revisions for Traffic Splitting\nAzure Container Apps provides a powerful mechanism for managing application changes using revisions and labels. Revisions allow you to create immutable snapshots of your application, and labels enable you to manage and route traffic between these revisions.\nStep-by-Step Implementation\nCreate Initial Deployment, label new revision and set traffic weight for the label to 100%\nsrinmantest.azurecr.io/bookstoreapi:v1 and srinmantest.azurecr.io/bookstoreapi:v2 are the images for the web application. Please change this to your image.\nFirst, deploy your initial version of the web application to Azure Container Apps.\nuami_id=$(az identity show --resource-group academorg --name acaacrpulluami --query id --output tsv)\nAssign AcrPull role for this identity to the ACR where the image is stored.\naz containerapp create\nbookstoreapi --resource-group academorg\nacademoenvw2con --workload-profile-name\nsrinmantest.azurecr.io/bookstoreapi:v1 --target-port\nexternal --revisions-mode multiple --revision-suffix v1\nproperties.configuration.ingress.fqdn --registry-identity\n--registry-server srinmantest.azurecr.io --min-replicas\naz containerapp revision label\nbookstoreapi --resource-group academorg\naz containerapp ingress traffic\nbookstoreapi --resource-group academorg --label-weight\naz containerapp revision list\nbookstoreapi --resource-group academorg\nDeploy the new version of your web application. This will create a new revision.\naz containerapp update\nbookstoreapi --resource-group academorg\nsrinmantest.azurecr.io/bookstoreapi:v2 --min-replicas\naz containerapp revision label\nbookstoreapi --resource-group academorg\nList the revisions to get the names of the current and new revisions.\naz containerapp revision list\nmy-web-app --resource-group my-resource-group\n\"[].{name:name, active:active}\"\naz containerapp revision list\nbookstoreapi --resource-group academorg \naz containerapp revision list\nbookstoreapi --resource-group academorg\nConfigure Traffic Splitting\nSplit the traffic between the stable and canary revisions. For example, route 90% of the traffic to the stable version and 10% to the canary version.\naz containerapp ingress traffic\nmy-web-app --resource-group my-resource-group --traffic-weight\naz containerapp ingress traffic\nacademorg --label-weight\nMonitor the performance and stability of the canary revision using Azure Monitor or Application Insights. Ensure that the new version is functioning correctly and does not introduce any issues.\nGradually Increase Traffic\nIf the canary revision is stable, gradually increase the traffic to the canary revision until it receives 100% of the traffic.\naz containerapp ingress traffic\nmy-web-app --resource-group my-resource-group --traffic-weight\naz containerapp ingress traffic\nmy-web-app --resource-group my-resource-group --traffic-weight\naz containerapp ingress traffic\nacademorg --label-weight\naz containerapp ingress traffic\nacademorg --label-weight\nPromote Canary to Stable\nOnce the canary revision is fully validated, promote it to the stable label. Label swap helps with this.\naz containerapp revision label swap\nbookstoreapi --resource-group academorg\nDeactive old revision\nOld revision is not needed anymore. Deactivate it.\naz containerapp revision deactivate\nbookstoreapi --resource-group academorg\nBy leveraging labels and revisions in Azure Container Apps, you can effectively manage application changes and perform canary deployments with traffic splitting. This approach allows you to gradually roll out new versions of your application, monitor their performance, and ensure stability before fully deploying them to all users. This method simplifies the deployment process and reduces the risk of introducing issues in production.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog6/",
        "title": "Azure Container App - Traffic Splitting",
        "section": "Part 1 of 1",
        "word_count": 659
      }
    },
    {
      "text": "Managing certificates and creating custom domains for Azure services can be a daunting task for many developers and IT professionals. Ensuring secure and reliable access to your services while maintaining control over your domain can be challenging. How can one approach creating custom domains for Azure services, and what steps are involved in verifying domain ownership and routing traffic correctly?\nAzure provides a streamlined process for creating custom domains, making it easier to manage certificates and ensure secure access to your services. When configuring a custom domain for Azure API Management (APIM), for example, Azure requires you to add specific DNS records to verify domain ownership and route traffic correctly.\nSteps for Configuring Custom Domain in APIM:\n: Azure asks you to add a TXT record for\napimuid.apim.srinman.com\nto verify that you own the domain. This step ensures that only authorized users can configure the domain.\n: You need to add a CNAME record for\nto route traffic to the APIM service. This ensures that requests to your custom domain are correctly directed to the Azure APIM endpoint.\nAzure simplifies the process by providing clear instructions and the necessary values for these DNS records. Once the records are added and verified, your custom domain will be active and securely routed to the Azure service.\nImportant information:\nPlease notice in the \"Traffic Routing\" section, the CNAME record is where the magic happens, as it effectively glues your custom domain to the Azure service without the custom domain.\nAzure simplifies the process by providing clear instructions and the necessary values for these DNS records. Once the records are added and verified, your custom domain will be active and securely routed to the Azure service.\nExample DNS Configuration:\nTXT apimuid.apim.srinman.com <verification-code-from-azure>\nCNAME apim.srinman.com <your-apim-service-name>.azure-api.net\nOther Azure Services Supporting Custom Domains\nIn addition to APIM, several other Azure services support custom domains, including:\n: Host your web apps with custom domains and SSL certificates.\n: Serve static content from custom domains.\n: Use custom domains for global load balancing and secure content delivery.\n: Deliver content with custom domains and SSL.\nFor more detailed information, refer to the following Microsoft documentation:\nConfigure a custom domain name for Azure API Management\nAdd a custom domain to your Azure App Service\nAzure Front Door custom domain configuration\nAzure CDN custom domain setup\nBy following these guidelines, you can efficiently manage certificates and create custom domains for various Azure services, ensuring secure and reliable access to your applications.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog5/",
        "title": "Custom domain and APIM",
        "section": "Part 1 of 1",
        "word_count": 411
      }
    },
    {
      "text": "Many organizations rely on the SFTP (Secure File Transfer Protocol) for secure and reliable file transfers. However, managing and scaling SFTP servers can be complex and resource-intensive. Additionally, integrating SFTP with modern cloud storage solutions like Azure Blob Storage can be challenging, requiring custom solutions and additional infrastructure.\nAzure Storage account's SFTP feature simplifies this process by providing a fully managed SFTP endpoint that directly integrates with Azure Blob Storage. This feature allows users to securely upload, download, and manage files using the SFTP protocol without the need to manage any underlying infrastructure. By leveraging Azure's scalable and resilient storage, organizations can ensure high availability and durability for their data while maintaining the simplicity and security of SFTP.\nUse this link to understand how to leverage SFTP server\nhttps://github.com/srinman/azureexamples/tree/main/blobsftp",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog4/",
        "title": "SFTP and Azure Blob Storage",
        "section": "Part 1 of 1",
        "word_count": 129
      }
    },
    {
      "text": "You have an application running in Azure Kubernetes Service (AKS) that needs to access other Azure services. You don't want to use connection string with secrets in the code. You don't want to use service principal with secrets in the code. You want to use managed identities to authenticate to the Azure services. How do you do this in AKS?\nTwo types of identities:\nApplication identities: More ownership, need to create secrets for authentication.\nManaged identities: Less ownership, no need to create secrets for authentication. (Azure platform manages the secrets).\nWithin managed identities, there are two types:\nSystem assigned managed identity: Managed by Azure platform. Attached to the resource. The resource is the owner of the identity.\nUser assigned managed identity: Managed by Azure platform. Attached to the resource. The resource is not the owner of the identity. The identity can be attached to multiple resources.\nAzure VMs, Azure App Services, Azure Container Apps, Azure Functions, etc. These are all first party services in Azure. User assigned managed identities can be used in these services (or attached to these services).\nServices such as AKS can host multiple applications and services. Kubernetes service accounts are the identity mechanism in Kubernetes for applications. Workload identity is a way to map managed identities to Kubernetes service accounts.\nFederated credential creates this linkage:\naz identity federated-credential create --name fedcredentialname --identity-name USER_ASSIGNED_IDENTITY_NAME --resource-group --issuer \"${AKS_OIDC_ISSUER}\" --subject system:serviceaccount:namespace:serviceaccount --audience api://AzureADTokenExchange\nLet's use the following scenario to understand the workload identity.\nUpload blob to a storage account from a pod in AKS. This application is running in a pod in AKS. The application needs to authenticate to the storage account to upload the blob. The application is using the managed identity to authenticate to the storage account. This applican can also be run in local machine.\nRefer this github repo for step by step instructions:\nhttps://github.com/srinman/azureexamples/tree/main/aks/workloadidentity\nThis repo has pythonblobdac.py and pythonblobwic.py files. The pythonblobdac.py file uses the Azure default credentials to authenticate to the Azure services. The pythonblobwic.py file uses the workload identity to authenticate to the Azure services.\nCreate a storage account and a container called 'workloadidentity' in the storage account.\nProvide IAM role 'Storage Blob Data Contributor' to the user assigned managed identity and for the user who is running the python code in local machine.\nDefaultAzureCredential\nWhen this program is called from command line directly with the command below\npython pythonblobdac.py\nIt uses the developers to use their own accounts to authenticate to Azure services.\nfrom azure.identity import DefaultAzureCredential\ndefault_credential = DefaultAzureCredential()\nblob_service_client = BlobServiceClient(account_url, credential=default_credential)\nDefaultAzureCredential attempts to authenticate via the following mechanisms, in this order, stopping when one succeeds: (read the link below for more details)\nhttps://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python#defaultazurecredential\nIn case of calling program from command line, it uses the developers account to authenticate to the Azure services. It's number#4 in the doc (excerpt below)\nAzure CLI - If a user has signed in via the Azure CLI az login command, DefaultAzureCredential will authenticate as that user.\nWhen the same code is called from a pod in AKS, it uses workload identity to authenticate to the Azure services. It's number#2 in the doc (excerpt below)\nWorkload Identity - If the application is deployed to Azure Kubernetes Service with Managed Identity enabled, DefaultAzureCredential will authenticate with it.\nPython code remains same. so in a way, the code is portable. It can be run in local machine or in AKS. The only difference is how the code authenticates to the Azure services.\nIn this program, only difference is that it uses WorkloadIdentityCredential() vs DefaultAzureCredential(). The rest of the code remains same. This also means that the code can only be run in AKS. It cannot be run in local machine.\nhttps://blog.identitydigest.com/azuread-federate-mi/\nhttps://github.com/Azure/azure-sdk-for-python/tree/azure-identity_1.17.1/sdk/identity/azure-identity\nhttps://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.workloadidentitycredential?view=azure-python\nhttps://azure.github.io/azure-sdk-for-python/\nhttps://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog3/",
        "title": "AKS Workload Identity in Practice",
        "section": "Part 1 of 1",
        "word_count": 620
      }
    },
    {
      "text": "Many open-source projects, such as Apache Flink and Apache Spark, as well as various commercial products, rely heavily on the Kubernetes API for deployment and orchestration. However, general containerized applications do not necessarily require direct interaction with the Kubernetes API and can be abstracted using platforms like Azure Container Apps or custom-built platforms. This abstraction poses a significant challenge for OSS projects and other Kubernetes-dependent applications, as they cannot leverage these simplified platforms without losing essential Kubernetes-specific functionalities. The lack of a unified deployment strategy that accommodates both Kubernetes-dependent and abstracted containerized applications creates operational inefficiencies and complicates the deployment process.\nTo address this challenge, one approach is Kubernetes as a dedicated service specifically for projects that require direct interaction with the Kubernetes API. This approach involves:\nDedicated Kubernetes Service\n: Establishing a dedicated Kubernetes service such as AKS that provides full Kubernetes API support for OSS projects and other Kubernetes-dependent applications.\nSeparation of Concerns\n: Clearly separating the deployment environments for Kubernetes-dependent applications and general containerized applications, allowing each to use the most appropriate platform.\nOptimized Resource Allocation\n: Allocating resources and infrastructure specifically tailored to the needs of Kubernetes-dependent applications, ensuring optimal performance and scalability.\nSimplified Management\n: Simplifying the management and orchestration of Kubernetes-dependent applications by providing a consistent and fully supported Kubernetes environment.\nAdditional Guidance for OSS Projects Depending on Kubernetes API\nDocumentation and Best Practices\nProvide comprehensive documentation on how to deploy and manage the OSS project on Kubernetes.\nInclude best practices for configuration, scaling, and monitoring.\nOffer Helm charts to simplify the deployment process.\nEnsure the Helm charts are well-maintained and regularly updated.\nDevelop Kubernetes Operators to automate the management of the OSS project.\nUse the Operator Framework to build, test, and deploy Operators.\nFoster a community around the OSS project to share knowledge and support.\nUse forums, mailing lists, and chat channels to facilitate communication.\nInclude instructions for setting up monitoring, logging, and tracing.\nUse tools like Prometheus, Grafana, and Jaeger for observability.\nProvide sample applications and use cases to demonstrate the capabilities of the OSS project.\nInclude step-by-step guides for deploying and running these samples on Kubernetes.\nBy following these guidelines along with Azure Policy and Azure Policy for AKS, organizations can enforce governance and security requirements for Kubernetes-dependent applications while providing the flexibility needed for different types of applications. This ensures that applications remain compliant with organizational standards and regulatory requirements, enhancing overall security and operational efficiency.\nRole of AKS Automatic\nAzure Kubernetes Service (AKS) Automatic can play a critical role in this use case by providing automated cluster management and operational efficiencies. AKS Automatic offers:\n: Ensures that the Kubernetes clusters are always up-to-date with the latest security patches and features without manual intervention.\n: Automatically scales the cluster based on workload demands, ensuring optimal resource utilization and performance.\nMonitoring and Diagnostics\n: Provides built-in monitoring and diagnostics to help identify and resolve issues quickly.\n: Enhances security by automatically applying security patches and updates, reducing the risk of vulnerabilities.\n: Optimizes cost by scaling resources up or down based on actual usage, preventing over-provisioning.\nBy leveraging AKS Automatic, organizations can further simplify the management of their dedicated Kubernetes service, ensuring that OSS projects and other Kubernetes-dependent applications are always running on a secure, scalable, and up-to-date infrastructure.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog7/",
        "title": "Kubernetes as a Service",
        "section": "Part 1 of 1",
        "word_count": 548
      }
    },
    {
      "text": "You need to run containers in Azure. What are the options available to you? What are the pros and cons of each option? How do you choose the right option for your use case?\nIn order to run a program, you need a source code and a runtime environment. This runtime environment can be a virtual machine with an operating system and whatever dependencies are needed to run the program. This is the traditional way of running programs. However, there are other ways to run programs. One of these ways is to use containers. Containers are a way to package up a program and all of its dependencies into a single package. This package can then be run on any machine that has a container runtime installed. This makes it easy to run programs in a consistent way across different machines. Containers are lightweight and fast to start up, making them a good choice for running programs in the cloud.\nThis container packaging has evolved into a standard which is OCI (Open Container Initiative) Image and Distribution Spec. This standard defines how containers should be packaged and run.\nhttps://opencontainers.org/posts/blog/2024-03-13-image-and-distribution-1-1/\nWith a common/standard image spec, there is also a common/standard runtime spec. This is the OCI Runtime Spec. This spec defines how containers should be run.\nhttps://opencontainers.org/posts/blog/2024-02-18-oci-runtime-spec-v1-2/#what-is-the-oci-runtime-spec\nThere are several reference implementations of the OCI runtime spec. Within this reference implementation, runtime implementation can be broadly categorized into two types: low-level and high-level.\nLow-level runtime implementations are responsible for the core functionalities of container management. They handle the creation, execution, and lifecycle management of containers at a granular level. These implementations interact directly with the operating system's kernel and utilize system calls to manage namespaces, cgroups, and other low-level resources. Examples of low-level runtimes include\n, which are designed to be lightweight and efficient, providing the essential building blocks for container execution.\nHigh-level runtime implementations, on the other hand, build upon the capabilities provided by low-level runtimes. They offer additional features and abstractions that simplify container management and orchestration. High-level runtimes often integrate with container orchestration platforms like Kubernetes and provide functionalities such as image management, network configuration, and volume handling. Examples of high-level runtimes include\n, which provide a more comprehensive and user-friendly interface for managing containers in a production environment.\nContainer Options in Azure\nAzure offers several first-party options for running containers. These are Azure Container Instances (ACI), Azure Kubernetes Service (AKS), and Azure Container Apps (ACA). Each of these services has its own use cases and features. Depending on your requirements, you can choose the service that best fits your needs.\nAzure Container Instances (ACI)\nAzure Container Instances is a service that allows you to run containers without having to manage the underlying infrastructure. You can run containers on demand and only pay for the resources that you use. This makes it easy to run containers in the cloud without having to worry about managing servers. In essence, ACI is like running\nbut in the Azure cloud, providing a simple and efficient way to deploy and manage containers.\nAzure Kubernetes Service (AKS)\nAzure Kubernetes Service (AKS) is a managed Kubernetes service that enables you to run containers at scale. Kubernetes is an open-source container orchestration platform that allows you to manage containers within a cluster. AKS simplifies the deployment and management of Kubernetes clusters in Azure.\nOne of the key advantages of AKS is its access to the Kubernetes API, which allows you to build and customize your own deployment options. While the deployment and management of AKS itself are handled via Azure Resource Manager (ARM) APIs, the deployment and management of containers within AKS are managed through the Kubernetes API. This differs from Azure Container Apps (ACA), where the Container Apps service is directly integrated as a first-party API within ARM.\nAzure Container Apps (ACA)\nAzure Container Apps is a service that allows you to run containers in a serverless way. It support consumption based pricing and also workload profiles which let you choose VMs based on your workload. Autoscaling of replicas (of your application) and also scaling of VMs is supported and seamless. mTLS (in-transit encryption) is supported. Resiliency options such as retry policies, circuit breakers, and timeouts are supported. It also supports monitoring and logging. It is a fully managed service and opinionated. If you are happy with features in ACA and don't want to customize, then ACA is a good choice. ACA is a good choice for developers who want to focus on their application and not worry about the underlying infrastructure. ACA service will continue to evolve and add more features.\nWhile there are other options, these are the main ones that are commonly used.",
      "metadata": {
        "author": "Sridher Manivel",
        "category": "container",
        "url": "https://blog.srinman.com/blog1/",
        "title": "Azure Container Options",
        "section": "Part 1 of 1",
        "word_count": 782
      }
    }
  ]
}